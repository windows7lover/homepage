I"Ñ+<h2 class="bibliography">Journal Articles</h2>
<ol class="bibliography"><li><span id="bollapragada2022nonlinear"> <b>Nonlinear acceleration of momentum and primal-dual algorithms</b>.<div class="csl-block"> <span style="font-variant: small-caps">R. Bollapragada, D. Scieur, A. dâ€™Aspremont</span>.</div><div class="csl-block"> In <i>Mathematical Programming</i> (2022).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="bollapragada2022nonlinear-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#bollapragada2022nonlinear-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#bollapragada2022nonlinear-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1810.04539" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="bollapragada2022nonlinear-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We describe convergence acceleration schemes for multistep optimization algorithms. The extrapolated solution is written as a nonlinear average of the iterates produced by the original optimization method. Our analysis does not need the underlying fixed-point operator to be symmetric, hence handles e.g. algorithms with momentum terms such as Nesterovâ€™s accelerated method, or primal-dual methods. The weights are computed via a simple linear system and we analyze performance in both online and offline modes. We use Crouzeixâ€™s conjecture to show that acceleration performance is controlled by the solution of a Chebyshev problem on the numerical range of a non-symmetric operator modeling the behavior of iterates near the optimum. Numerical experiments are detailed on logistic regression problems.
                </div>
			</div>
		

		<div id="bollapragada2022nonlinear-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bollapragada2022nonlinear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonlinear acceleration of momentum and primal-dual algorithms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bollapragada, Raghu and Scieur, Damien and d'Aspremont, Alexandre}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2020regularized"> <b>Regularized nonlinear acceleration.</b><div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, A. dâ€™Aspremont, F. Bach</span>.</div><div class="csl-block"> In <i>Mathematical Programming</i> (2020).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2020regularized-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2020regularized-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2020regularized-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://link.springer.com/article/10.1007/s10107-018-1319-8" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2020regularized-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We describe a convergence acceleration technique for unconstrained optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.
                </div>
			</div>
		

		<div id="scieur2020regularized-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">scieur2020regularized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Regularized nonlinear acceleration.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{179}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="papavasiliou2017application"> <b>Application of Stochastic Dual Dynamic Programming to the Real-Time Dispatch of Storage under Renewable Supply Uncertainty</b>.<div class="csl-block"> <span style="font-variant: small-caps">A. Papavasiliou, Y. Mou, L. Cambier, D. Scieur</span>.</div><div class="csl-block"> In <i>IEEE Transactions on Sustainable Energy</i> (2017).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="papavasiliou2017application-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#papavasiliou2017application-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#papavasiliou2017application-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			
				<a href="http://localhost:4000/pdf/papers/Application of Stochastic Dual Dynamic Programming to the Real-Time Dispatch of Storage under Renewable Supply Uncertainty.pdf" target="_blank">[PDF]</a>&nbsp;
			
		
				
				
		

			<div id="papavasiliou2017application-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    This paper presents a multi-stage stochastic programming formulation of transmission-constrained economic dispatch subject to multi-area renewable production uncertainty, with a focus on optimizing the dispatch of storage in real-time operations. This problem is resolved using stochastic dual dynamic programming. The applicability of the proposed approach is demonstrated on a realistic case study of the German power system calibrated against the solar and wind power integration levels of 2013-2014, with a 24-hour horizon and 15-minute time step. The value of the stochastic solution relative to the cost of a deterministic policy amounts to 1.1%, while the value of perfect foresight relative to the cost of the stochastic programming policy amounts to 0.8%. The relative performance of various alternative real-time dispatch policies is analyzed, and the sensitivity of the results is explored.
                </div>
			</div>
		

		<div id="papavasiliou2017application-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">papavasiliou2017application</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Application of Stochastic Dual Dynamic Programming to the Real-Time Dispatch of Storage under Renewable Supply Uncertainty}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Papavasiliou, Anthony and Mou, Yuting and Cambier, L{\'e}opold and Scieur, Damien}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Sustainable Energy}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li></ol>
<h2 class="bibliography">Books</h2>
<ol class="bibliography"><li><span id="d2021acceleration"> <b>Acceleration methods</b>.<div class="csl-block"> <span style="font-variant: small-caps">A. dâ€™Aspremont, D. Scieur, A. Taylor, others</span>.</div><div class="csl-block"> In <i>Foundations and Trends in Optimization</i> (2021).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="d2021acceleration-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#d2021acceleration-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#d2021acceleration-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2101.09545" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="d2021acceleration-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    This monograph covers some recent advances in a range of acceleration techniques frequently used in convex optimization. We first use quadratic optimization problems to introduce two key families of methods, namely momentum and nested optimization schemes. They coincide in the quadratic case to form the Chebyshev method. We discuss momentum methods in detail, starting with the seminal work of Nesterov and structure convergence proofs using a few master templates, such as that for optimized gradient methods, which provide the key benefit of showing how momentum methods optimize convergence guarantees. We further cover proximal acceleration, at the heart of the Catalyst and Accelerated Hybrid Proximal Extragradient frameworks, using similar algorithmic patterns. Common acceleration techniques rely directly on the knowledge of some of the regularity parameters in the problem at hand. We conclude by discussing restart schemes, a set of simple techniques for reaching nearly optimal convergence rates while adapting to unobserved regularity parameters.
                </div>
			</div>
		

		<div id="d2021acceleration-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@book</span><span class="p">{</span><span class="nl">d2021acceleration</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Acceleration methods}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{dâ€™Aspremont, Alexandre and Scieur, Damien and Taylor, Adrien and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Foundations and Trends in Optimization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Now Publishers, Inc.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li></ol>
<h2 class="bibliography">Theses</h2>
<ol class="bibliography"><li><span id="scieur2018acceleration"> <b>Acceleration in Optimization</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur</span> (2018).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2018acceleration-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2018acceleration-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2018acceleration-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			
				<a href="http://localhost:4000/pdf/papers/Acceleration in Optimization.pdf" target="_blank">[PDF]</a>&nbsp;
			
		
				
				
		

			<div id="scieur2018acceleration-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    In many different fields such as optimization, the performance of a method is often characterized
by its rate of convergence. However, accelerating an algorithm requires a lot of knowledge about
the problemâ€™s structure, and such improvement is done on a case-by-case basis. Many accelerated
schemes have been developed in the past few decades and are massively used in practice. Despite
their simplicity, such methods are usually based on purely algebraic arguments and often do not
have an intuitive explanation.
Recently, heavy work has been done to link accelerated algorithms with other fields of science, such as control theory or differential equations. However, these explanations often rely on
complex arguments, usually using non-conventional tools in their analysis.
One of the contributions of this thesis is a tentative explanation of optimization algorithms
using the theory of integration methods, which has been well studied and enjoys a solid theoretical
analysis. In particular, we will show that optimization scheme are special instance of integration
methods when integrating the classical gradient flow. With standard arguments, we intuitively
explain the origin of acceleration.
On the other hand, accelerated methods usually need additional parameters in comparison
with slower one, which are in most cases difficult to estimate. In addition, these schemes are
designed for one particular setting and cannot be used elsewhere.
In this thesis, we explore a new approach for accelerating optimization algorithms, which
uses generic acceleration arguments. In numerical analysis, these tools have been developed for
accelerating sequences of scalars or vectors, by building on the side another sequence with a
better convergence rate. These methods can be combined with an iterative algorithm, speeding
it up in most cases. In practice, extrapolation schemes are not widely used due to their lack of
theoretical guarantees and their instability. We will extend these methods by regularizing them,
allowing a deeper theoretical analysis and stronger convergence results, especially when applied
to optimization methods.
                </div>
			</div>
		

		<div id="scieur2018acceleration-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">scieur2018acceleration</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Acceleration in Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2015master"> <b>Global Complexity Analysis For The Second-Order Methods</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur</span> (2015).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2015master-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2015master-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2015master-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			
				<a href="http://localhost:4000/pdf/papers/Global Complexity Analysis For The Second-Order Methods.pdf" target="_blank">[PDF]</a>&nbsp;
			
		
				
				
		

			<div id="scieur2015master-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    The goal of this master thesis will be firstly to analyze with precision the behavior of the cubic regularization of Newtonâ€™s method on strongly convex functions. During this analysis we will find that the regularisation does not work as well as expected when the function is also smooth, unlike the gradient method. We will thus propose some variants of the original algorithm in order to have better global performances.
                </div>
			</div>
		

		<div id="scieur2015master-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">scieur2015master</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Global Complexity Analysis For The Second-Order Methods}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Universit\'e catholique de Louvain}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li></ol>
<h2 class="bibliography">Conference Articles</h2>
<ol class="bibliography"><li><span id="goujaud2022super"> <b>Super-acceleration with cyclical step-sizes</b>.<div class="csl-block"> <span style="font-variant: small-caps">B. Goujaud, D. Scieur, A. Dieuleveut, A. B. Taylor, F. Pedregosa</span>.</div><div class="csl-block"> In <i>International Conference on Artificial Intelligence and Statistics</i> (2022).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="goujaud2022super-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#goujaud2022super-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#goujaud2022super-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2106.09687" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="goujaud2022super-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We develop a convergence-rate analysis of momentum with cyclical step-sizes. We show that under some assumption on the spectral gap of Hessians in machine learning, cyclical step-sizes are provably faster than constant step-sizes. More precisely, we develop a convergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds. We further propose a systematic approach to design optimal first order methods for quadratic minimization with a given spectral structure. Finally, we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our findings through benchmarks on least squares and logistic regression problems.
                </div>
			</div>
		

		<div id="goujaud2022super-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">goujaud2022super</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Super-acceleration with cyclical step-sizes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Scieur, Damien and Dieuleveut, Aymeric and Taylor, Adrien B and Pedregosa, Fabian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3028--3065}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="cunha2021only"> <b>Only tails matter: Average-Case Universality and Robustness in the Convex Regime</b>.<div class="csl-block"> <span style="font-variant: small-caps">L. Cunha, G. Gidel, F. Pedregosa, C. Paquette, D. Scieur</span>.</div><div class="csl-block"> In <i>International Conference on Machine Learning</i> (2022).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="cunha2021only-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#cunha2021only-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#cunha2021only-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2206.09901" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="cunha2021only-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    The recently developed average-case analysis of optimization methods allows a more fine-grained and representative convergence analysis than usual worst-case results. In exchange, this analysis requires a more precise hypothesis over the data generating process, namely assuming knowledge of the expected spectral distribution (ESD) of the random matrix associated with the problem. This work shows that the concentration of eigenvalues near the edges of the ESD determines a problemâ€™s asymptotic average complexity. This a priori information on this concentration is a more grounded assumption than complete knowledge of the ESD. This approximate concentration is effectively a middle ground between the coarseness of the worst-case scenario convergence and the restrictive previous average-case analysis. We also introduce the Generalized Chebyshev method, asymptotically optimal under a hypothesis on this concentration and globally optimal when the ESD follows a Beta distribution. We compare its performance to classical optimization algorithms, such as gradient descent or Nesterovâ€™s scheme, and we show that, in the average-case context, Nesterovâ€™s method is universally nearly optimal asymptotically.
                </div>
			</div>
		

		<div id="cunha2021only-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cunha2021only</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Only tails matter: Average-Case Universality and Robustness in the Convex Regime}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cunha, Leonardo and Gidel, Gauthier and Pedregosa, Fabian and Paquette, Courtney and Scieur, Damien}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2021connecting"> <b>Connecting Sphere Manifolds Hierarchically for Regularization</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, Y. Kim</span>.</div><div class="csl-block"> In <i>International Conference on Machine Learning</i> (2021).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2021connecting-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2021connecting-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2021connecting-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2106.13549" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2021connecting-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    This paper considers classification problems with hierarchically organized classes. We force the classifier (hyperplane) of each class to belong to a sphere manifold, whose center is the classifier of its super-class. Then, individual sphere manifolds are connected based on their hierarchical relations. Our technique replaces the last layer of a neural network by combining a spherical fully-connected layer with a hierarchical layer. This regularization is shown to improve the performance of widely used deep neural network architectures (ResNet and DenseNet) on publicly available datasets (CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).
                </div>
			</div>
		

		<div id="scieur2021connecting-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scieur2021connecting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Connecting Sphere Manifolds Hierarchically for Regularization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and Kim, Youngsung}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9399--9409}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2021generalization"> <b>Generalization of Quasi-Newton methods: application to robust symmetric multisecant updates</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, L. Liu, T. Pumir, N. Boumal</span>.</div><div class="csl-block"> In <i>International Conference on Artificial Intelligence and Statistics</i> (2021).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2021generalization-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2021generalization-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2021generalization-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2011.03358" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2021generalization-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Quasi-Newton techniques approximate the Newton step by estimating the Hessian using the so-called secant equations. Some of these methods compute the Hessian using several secant equations but produce non-symmetric updates. Other quasi-Newton schemes, such as BFGS, enforce symmetry but cannot satisfy more than one secant equation. We propose a new type of quasi-Newton symmetric update using several secant equations in a least-squares sense. Our approach generalizes and unifies the design of quasi-Newton updates and satisfies provable robustness guarantees.
                </div>
			</div>
		

		<div id="scieur2021generalization-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scieur2021generalization</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalization of Quasi-Newton methods: application to robust symmetric multisecant updates}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and Liu, Lewis and Pumir, Thomas and Boumal, Nicolas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{550--558}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="kerdreux2021affine"> <b>Affine invariant analysis of frank-wolfe on strongly convex sets</b>.<div class="csl-block"> <span style="font-variant: small-caps">T. Kerdreux, L. Liu, S. Lacoste-Julien, D. Scieur</span>.</div><div class="csl-block"> In <i>International Conference on Machine Learning</i> (2021).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="kerdreux2021affine-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#kerdreux2021affine-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#kerdreux2021affine-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2011.03351" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="kerdreux2021affine-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    It is known that the Frank-Wolfe (FW) algorithm, which is affine-covariant, enjoys accelerated convergence rates when the constraint set is strongly convex. However, these results rely on norm-dependent assumptions, usually incurring non-affine invariant bounds, in contradiction with FWâ€™s affine-covariant property. In this work, we introduce new structural assumptions on the problem (such as the directional smoothness) and derive an affine invariant, norm-independent analysis of Frank-Wolfe. Based on our analysis, we propose an affine invariant backtracking line-search. Interestingly, we show that typical backtracking line-searches using smoothness of the objective function surprisingly converge to an affine invariant step size, despite using affine-dependent norms in the step sizeâ€™s computation. This indicates that we do not necessarily need to know the setâ€™s structure in advance to enjoy the affine-invariant accelerated rate.
                </div>
			</div>
		

		<div id="kerdreux2021affine-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kerdreux2021affine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Affine invariant analysis of frank-wolfe on strongly convex sets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kerdreux, Thomas and Liu, Lewis and Lacoste-Julien, Simon and Scieur, Damien}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5398--5408}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2020universal"> <b>Universal average-case optimality of polyak momentum</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, F. Pedregosa</span>.</div><div class="csl-block"> In <i>International conference on machine learning</i> (2020).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2020universal-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2020universal-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2020universal-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2002.04664" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2020universal-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Polyak momentum (PM), also known as the heavy-ball method, is a widely used optimization method that enjoys an asymptotic optimal worst-case complexity on quadratic objectives. However, its remarkable empirical success is not fully explained by this optimality, as the worst-case analysis â€“ contrary to the average-case â€“ is not representative of the expected complexity of an algorithm. In this work we establish a novel link between PM and the average-case analysis. Our main contribution is to prove that any optimal average-case method converges in the number of iterations to PM, under mild assumptions. This brings a new perspective on this classical method, showing that PM is asymptotically both worst-case and average-case optimal. 
                </div>
			</div>
		

		<div id="scieur2020universal-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scieur2020universal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Universal average-case optimality of polyak momentum}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and Pedregosa, Fabian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International conference on machine learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8565--8572}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="pedregosa2020acceleration"> <b>Acceleration through spectral density estimation</b>.<div class="csl-block"> <span style="font-variant: small-caps">F. Pedregosa, D. Scieur</span>.</div><div class="csl-block"> In <i>International Conference on Machine Learning</i> (2020).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="pedregosa2020acceleration-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#pedregosa2020acceleration-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#pedregosa2020acceleration-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2002.04756" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="pedregosa2020acceleration-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We develop a framework for the average-case analysis of random quadratic problems and derive algorithms that are optimal under this analysis. This yields a new class of methods that achieve acceleration given a model of the Hessianâ€™s eigenvalue distribution. We develop explicit algorithms for the uniform, Marchenko-Pastur, and exponential distributions. These methods are momentum-based algorithms, whose hyper-parameters can be estimated without knowledge of the Hessianâ€™s smallest singular value, in contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum. Through empirical benchmarks on quadratic and logistic regression problems, we identify regimes in which the the proposed methods improve over classical (worst-case) accelerated methods.
                </div>
			</div>
		

		<div id="pedregosa2020acceleration-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pedregosa2020acceleration</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Acceleration through spectral density estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pedregosa, Fabian and Scieur, Damien}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7553--7562}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="azizian2020accelerating"> <b>Accelerating smooth games by manipulating spectral shapes</b>.<div class="csl-block"> <span style="font-variant: small-caps">Azizian WaÄ±Ìˆss, D. Scieur, I. Mitliagkas, S. Lacoste-Julien, G. Gidel</span>.</div><div class="csl-block"> In <i>International Conference on Artificial Intelligence and Statistics</i> (2020).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="azizian2020accelerating-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#azizian2020accelerating-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#azizian2020accelerating-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2001.00602" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="azizian2020accelerating-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We use matrix iteration theory to characterize acceleration in smooth games. We define the spectral shape of a family of games as the set containing all eigenvalues of the Jacobians of standard gradient dynamics in the family. Shapes restricted to the real line represent well-understood classes of problems, like minimization. Shapes spanning the complex plane capture the added numerical challenges in solving smooth games. In this framework, we describe gradient-based methods, such as extragradient, as transformations on the spectral shape. Using this perspective, we propose an optimal algorithm for bilinear games. For smooth and strongly monotone operators, we identify a continuum between convex minimization, where acceleration is possible using Polyakâ€™s momentum, and the worst case where gradient descent is optimal. Finally, going beyond first-order methods, we propose an accelerated version of consensus optimization.
                </div>
			</div>
		

		<div id="azizian2020accelerating-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">azizian2020accelerating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerating smooth games by manipulating spectral shapes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Azizian, Wa{\"\i}ss and Scieur, Damien and Mitliagkas, Ioannis and Lacoste-Julien, Simon and Gidel, Gauthier}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1705--1715}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="domingo2020average"> <b>Average-case acceleration for bilinear games and normal matrices</b>.<div class="csl-block"> <span style="font-variant: small-caps">C. Domingo-Enrich, F. Pedregosa, D. Scieur</span>.</div><div class="csl-block"> In <i>International Conference on Learning Representations</i> (2020).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="domingo2020average-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#domingo2020average-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#domingo2020average-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2010.02076" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="domingo2020average-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Advances in generative modeling and adversarial learning have given rise to renewed interest in smooth games. However, the absence of symmetry in the matrix of second derivatives poses challenges that are not present in the classical minimization framework. While a rich theory of average-case analysis has been developed for minimization problems, little is known in the context of smooth games. In this work we take a first step towards closing this gap by developing average-case optimal first-order methods for a subset of smooth games. We make the following three main contributions. First, we show that for zero-sum bilinear games the average-case optimal method is the optimal method for the minimization of the Hamiltonian. Second, we provide an explicit expression for the optimal method corresponding to normal matrices, potentially non-symmetric. Finally, we specialize it to matrices with eigenvalues located in a disk and show a provable speed-up compared to worst-case optimal algorithms. We illustrate our findings through benchmarks with a varying degree of mismatch with our assumptions.
                </div>
			</div>
		

		<div id="domingo2020average-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">domingo2020average</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Average-case acceleration for bilinear games and normal matrices}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Domingo-Enrich, Carles and Pedregosa, Fabian and Scieur, Damien}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="jelassi2020extra"> <b>Extra-gradient with player sampling for faster convergence in n-player games</b>.<div class="csl-block"> <span style="font-variant: small-caps">S. Jelassi, C. Domingo-Enrich, D. Scieur, A. Mensch, J. Bruna</span>.</div><div class="csl-block"> In <i>International Conference on Machine Learning</i> (2020).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="jelassi2020extra-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#jelassi2020extra-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#jelassi2020extra-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1905.12363" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="jelassi2020extra-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Data-driven modeling increasingly requires to find a Nash equilibrium in multi-player games, e.g. when training GANs. In this paper, we analyse a new extra-gradient method for Nash equilibrium finding, that performs gradient extrapolations and updates on a random subset of players at each iteration. This approach provably exhibits a better rate of convergence than full extra-gradient for non-smooth convex games with noisy gradient oracle. We propose an additional variance reduction mechanism to obtain speed-ups in smooth convex games. Our approach makes extrapolation amenable to massive multiplayer settings, and brings empirical speed-ups, in particular when using a heuristic cyclic sampling scheme. Most importantly, it allows to train faster and better GANs and mixtures of GANs.
                </div>
			</div>
		

		<div id="jelassi2020extra-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jelassi2020extra</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extra-gradient with player sampling for faster convergence in n-player games}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jelassi, Samy and Domingo-Enrich, Carles and Scieur, Damien and Mensch, Arthur and Bruna, Joan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4736--4745}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="bollapragada2019nonlinear"> <b>Nonlinear Acceleration of Primal-Dual Algorithms</b>.<div class="csl-block"> <span style="font-variant: small-caps">R. Bollapragada, D. Scieur, A. dâ€™Aspremont</span>.</div><div class="csl-block"> In <i>The 22nd International Conference on Artificial Intelligence and Statistics</i> (2019).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="bollapragada2019nonlinear-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#bollapragada2019nonlinear-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#bollapragada2019nonlinear-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			
				<a href="http://localhost:4000/pdf/papers/Nonlinear Acceleration of Primal-Dual Algorithms.pdf" target="_blank">[PDF]</a>&nbsp;
			
		
				
				
		

			<div id="bollapragada2019nonlinear-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We describe a convergence acceleration scheme for multi-step optimization algorithms. The extrapolated solution is written as a nonlinear average of the iterates produced by the original optimization algorithm. Our scheme does not need the underlying fixed-point operator to be symmetric, hence handles e.g. algorithms with momentum terms such as Nesterovâ€™s accelerated method, or primal-dual methods such as Chambolle-Pock. The weights are computed via a simple linear system and we analyze performance in both online and offline modes. We use Crouzeixâ€™s conjecture to show that acceleration is controlled by the solution of a Chebyshev problem on the numerical range of a nonsymmetric operator modelling the behavior of iterates near the optimum. Numerical experiments are detailed on image processing and logistic regression problems.
                </div>
			</div>
		

		<div id="bollapragada2019nonlinear-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bollapragada2019nonlinear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonlinear Acceleration of Primal-Dual Algorithms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bollapragada, Raghu and Scieur, Damien and dâ€™Aspremont, Alexandre}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 22nd International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{739--747}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="Kaneda2018optimal"> <b>Optimal Management of Storage for Offsetting Solar Power Uncertainty using Multistage Stochastic Programming</b>.<div class="csl-block"> <span style="font-variant: small-caps">T. Kaneda, B. Losseau, A. Papavasiliou, D. Scieur, L. Cambier, P. Henneaux, N. Leemput</span>.</div><div class="csl-block"> In <i>Proceedings of 20th Power Systems Computation Conference (to appear)</i> (2018).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="Kaneda2018optimal-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#Kaneda2018optimal-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#Kaneda2018optimal-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443062" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="Kaneda2018optimal-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Africa has recently engaged in implementing an aggressive renewable energy integration plan. A major challenge in the deployment of renewable power is the management of excess energy. The use of battery storage has been considered as a technically attractive solution. This paper tackles this operational problem using stochastic dual dynamical programming. We present an open-source MATLAB toolbox for multistage stochastic programming which employs stochastic dual dynamic programming. We use the toolbox in order to compare the stochastic solution to a greedy policy which operates batteries without future foresight as a benchmark. We consider a case study of storage management in Burkina Faso. We quantify the benefits of the stochastic solution and test the sensitivity of our results to the optimization horizon of the stochastic program.
                </div>
			</div>
		

		<div id="Kaneda2018optimal-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kaneda2018optimal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimal Management of Storage for Offsetting Solar Power Uncertainty using Multistage Stochastic Programming}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kaneda, Taku and Losseau, Bruno and Papavasiliou, Anthony and Scieur, Damien and Cambier, LÃ©opold and Henneaux, Pierre and Leemput, Niels}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of 20th Power Systems Computation Conference (to appear)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2017integration"> <b>Integration Methods and Accelerated Optimization Algorithms</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, V. Roulet, F. Bach, A. dâ€™Aspremont</span>.</div><div class="csl-block"> In <i>Advances In Neural Information Processing Systems</i> (2017).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2017integration-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2017integration-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2017integration-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1702.06751" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2017integration-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We show that accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. In comparison with recent advances in this vein, the differential equation considered here is the basic gradient flow and we show that multi-step schemes allow integration of this differential equation using larger step sizes, thus intuitively explaining acceleration results.
                </div>
			</div>
		

		<div id="scieur2017integration-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scieur2017integration</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Integration Methods and Accelerated Optimization Algorithms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and Roulet, Vincent and Bach, Francis and d'Aspremont, Alexandre}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances In Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2017nonlinear"> <b>Nonlinear Acceleration of Stochastic Algorithms</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, A. dâ€™Aspremont, F. Bach</span>.</div><div class="csl-block"> In <i>Advances In Neural Information Processing Systems</i> (2017).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2017nonlinear-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2017nonlinear-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2017nonlinear-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			
				<a href="http://localhost:4000/pdf/papers/Nonlinear Acceleration of Stochastic Algorithms.pdf" target="_blank">[PDF]</a>&nbsp;
			
		
				
				
		

			<div id="scieur2017nonlinear-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.
                </div>
			</div>
		

		<div id="scieur2017nonlinear-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scieur2017nonlinear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonlinear Acceleration of Stochastic Algorithms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances In Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2016regularized"> <b>Regularized Nonlinear Acceleration</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, A. dâ€™Aspremont, F. Bach</span>.</div><div class="csl-block"> In <i>Advances In Neural Information Processing Systems</i> (2016).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2016regularized-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2016regularized-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2016regularized-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1606.04133" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2016regularized-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.
                </div>
			</div>
		

		<div id="scieur2016regularized-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scieur2016regularized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Regularized Nonlinear Acceleration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances In Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{712--720}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li></ol>

<h2 id="arxiv">Arxiv</h2>
<ol class="bibliography"><li><span id="priol2021convergence"> <b>Convergence Rates for the MAP of an Exponential Family and Stochastic Mirror Descentâ€“an Open Problem</b>.<div class="csl-block"> <span style="font-variant: small-caps">R. L. Priol, F. Kunstner, D. Scieur, S. Lacoste-Julien</span>.</div><div class="csl-block"> In <i>arXiv preprint arXiv:2111.06826</i> (2021).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="priol2021convergence-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#priol2021convergence-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#priol2021convergence-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/2111.06826" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="priol2021convergence-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    We consider the problem of upper bounding the expected log-likelihood sub-optimality of the maximum likelihood estimate (MLE), or a conjugate maximum a posteriori (MAP) for an exponential family, in a non-asymptotic way. Surprisingly, we found no general solution to this problem in the literature. In particular, current theories do not hold for a Gaussian or in the interesting few samples regime. After exhibiting various facets of the problem, we show we can interpret the MAP as running stochastic mirror descent (SMD) on the log-likelihood. However, modern convergence results do not apply for standard examples of the exponential family, highlighting holes in the convergence literature. We believe solving this very fundamental problem may bring progress to both the statistics and optimization communities.
                </div>
			</div>
		

		<div id="priol2021convergence-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">priol2021convergence</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convergence Rates for the MAP of an Exponential Family and Stochastic Mirror Descent--an Open Problem}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Priol, R{\'e}mi Le and Kunstner, Frederik and Scieur, Damien and Lacoste-Julien, Simon}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2111.06826}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2019generalized"> <b>Generalized Framework for Nonlinear Acceleration</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur</span>.</div><div class="csl-block"> In <i>arXiv preprint arXiv:1903.08764</i> (2019).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2019generalized-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2019generalized-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2019generalized-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1903.08764" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2019generalized-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Nonlinear acceleration algorithms improve the performance of iterative methods, such as gradient descent, using the information contained in past iterates. However, their efficiency is still not entirely understood even in the quadratic case. In this paper, we clarify the convergence analysis by giving general properties that share several classes of nonlinear acceleration: Anderson acceleration (and variants), quasi-Newton methods (such as Broyden Type-I or Type-II, SR1, DFP, and BFGS) and Krylov methods (Conjugate Gradient, MINRES, GMRES). In particular, we propose a generic family of algorithms that contains all the previous methods and prove its optimal rate of convergence when minimizing quadratic functions. We also propose multi-secants updates for the quasi-Newton methods listed above. We provide a Matlab code implementing the algorithm.
                </div>
			</div>
		

		<div id="scieur2019generalized-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">scieur2019generalized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalized Framework for Nonlinear Acceleration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:1903.08764}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2018nonlinear"> <b>Nonlinear acceleration of cnns</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, E. Oyallon, A. dâ€™Aspremont, F. Bach</span>.</div><div class="csl-block"> In <i>arXiv preprint arXiv:1806.00370</i> (2018).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2018nonlinear-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2018nonlinear-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2018nonlinear-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1806.00370" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2018nonlinear-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    The Regularized Nonlinear Acceleration (RNA) algorithm is an acceleration method capable of improving the rate of convergence of many optimization schemes such as gradient descend, SAGA or SVRG. Until now, its analysis is limited to convex problems, but empirical observations shows that RNA may be extended to wider settings. In this paper, we investigate further the benefits of RNA when applied to neural networks, in particular for the task of image recognition on CIFAR10 and ImageNet. With very few modifications of exiting frameworks, RNA improves slightly the optimization process of CNNs, after training.
                </div>
			</div>
		

		<div id="scieur2018nonlinear-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">scieur2018nonlinear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonlinear acceleration of cnns}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and Oyallon, Edouard and d'Aspremont, Alexandre and Bach, Francis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:1806.00370}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li>
<li><span id="scieur2018online"> <b>Online Regularized Nonlinear Acceleration</b>.<div class="csl-block"> <span style="font-variant: small-caps">D. Scieur, E. Oyallon, A. dâ€™Aspremont, F. Bach</span>.</div><div class="csl-block"> In <i>arXiv preprint arXiv:1805.09639</i> (2018).</div></span>


<style>
  .vspace_defaut {
     margin-bottom: 1.2rem;
  }
</style>

<div class="marge" id="scieur2018online-materials">
  <div class="nav nav-pills" id="#myContent">
		
			<a data-toggle="collapse_custom" data-target="#scieur2018online-idAbstract" data-local="#myContent">[Abstract]</a>&nbsp;
		

		<a data-toggle="collapse_custom" data-target="#scieur2018online-idBibtex" data-local="#myContent">[Bibtex]</a>&nbsp;


		
			<a href="https://arxiv.org/abs/1805.09639" target="_blank">[PDF]</a>&nbsp;
		
				
				
		

			<div id="scieur2018online-idAbstract" class="collapse" data-local="#myContent">
                <div class="containerborder">
                    Regularized nonlinear acceleration (RNA) estimates the minimum of a function by post-processing iterates from an algorithm such as the gradient method. It can be seen as a regularized version of Anderson acceleration, a classical acceleration scheme from numerical analysis. The new scheme provably improves the rate of convergence of fixed step gradient descent, and its empirical performance is comparable to that of quasi-Newton methods. However, RNA cannot accelerate faster multistep algorithms like Nesterovâ€™s method and often diverges in this context. Here, we adapt RNA to overcome these issues, so that our scheme can be used on fast algorithms such as gradient methods with momentum. We show optimal complexity bounds for quadratics and asymptotically optimal rates on general convex minimization problems. Moreover, this new scheme works online, i.e., extrapolated solution estimates can be reinjected at each iteration, significantly improving numerical performance over classical accelerated methods.
                </div>
			</div>
		

		<div id="scieur2018online-idBibtex" class="collapse" data-local="#myContent">

<div>
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">scieur2018online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Regularized Nonlinear Acceleration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scieur, Damien and Oyallon, Edouard and d'Aspremont, Alexandre and Bach, Francis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:1805.09639}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>

			</div>
  </div>
	<p class="vspace_defaut">
	</p>	
</div>

</li></ol>
:ET