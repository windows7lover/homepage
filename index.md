---
layout: home
title: Home
nav_position: 1
---


## Short Bio
I am a research Scientist ar Samsung SAIL Montreal, in the MILA. The lab is headed by [Simon Lacoste-Julien](http://www.iro.umontreal.ca/~slacoste/). I focus my research on optimization for machine learning, especially accelerated and adaptive methods. I work closely with researchers  from Montreal and McGill Universities.


## Research interests
My main research interest is **convex optimization**, but I am also interested in **numerical analysis**. Here is a (non-exhaustive) selection of research topics:
- *Deterministic and Stochastic Optimization*,
- *Generic Acceleration Methods*,
- *Multisecant Quasi-Newton*,
- *Algorithms for Variational Inequalities*
- *Integration Algorithms for ODE* (and their links with optimization).

## Prior positions

### Post-doc (Princeton CS Department)
I worked one year at Princeton University in the CS department with [Elad Hazan](https://www.cs.princeton.edu/~ehazan/) and [Sanjeev Arora](https://www.cs.princeton.edu/~arora/). I collaborated with other researchers in ORFE ([Samy Jelassi](https://sjelassi.github.io/) and [Thomas Pumir](https://www.linkedin.com/in/thomas-pumir-ab77b936/)), as well as with [Nicolas Boumal](https://web.math.princeton.edu/~nboumal/) from the PACM. I worked on multisecant Quasi-Newton methods and stochastic algorithms for game theory.

### PhD Studies (INRIA/ENS)
I am a former PhD Student (from 2015 to 2018) of [Alexandre d'Aspremont](http://www.di.ens.fr/~aspremon/) and [Francis Bach](http://www.di.ens.fr/~fbach). I worked in the Sierra Team, part of the [Computer Science Department](http://www.di.ens.fr/) of [École Normale Supérieure Ulm](http://www.ens.fr/en). I also earned the [best thesis in datascience award from PSD University](https://espaces-numeriques.org/prix-de-these-psl-adeli-2019/)

My thesis, entitles Acceleration in Optimisation, focuses on links between acceleration and numerical analysis. You can find the manuscript [here](https://hal.archives-ouvertes.fr/tel-01887163/document).
The mains contribution is the design of the algorithm [Regularized Nonlinear Acceleration](https://arxiv.org/abs/1805.09639) - a generic way to improve the rate of convergence of many optimization methods.


### Master Studies (UCL)
I did my master thesis in optimization with my advisor [Yurii Nesterov](https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ). I graduated from [École Polytechnique de Louvain](https://uclouvain.be/fr/facultes/epl) in 2015 and got a master degree in Mathematical Engineering. 
At UCL I also collaborated with [Leopold Cambier](https://people.stanford.edu/lcambier/) and [Anthony Papavasiliou](http://perso.uclouvain.be/anthony.papavasiliou/public_html/) for the creation of the [FAST toolbox](https://web.stanford.edu/~lcambier/fast/). 
In the same time I worked with [Raphael Jungers](http://perso.uclouvain.be/raphael.jungers/content/home) and [Julien Hendrickx](https://perso.uclouvain.be/julien.hendrickx/) in the context of the [JSR Toolbox](http://fr.mathworks.com/matlabcentral/fileexchange/33202-the-jsr-toolbox).


## External Links
- [Github](https://github.com/windows7lover)
- [Google Scholar](https://scholar.google.fr/citations?user=hNscQzgAAAAJ&hl=fr)
- [LinkedIn](https://www.linkedin.com/in/damien-scieur-6873ba82/)
