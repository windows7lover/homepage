---
---
References
==========


@inproceedings{scieur2016regularized,
  title={Regularized Nonlinear Acceleration},
  author={Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
  booktitle={Advances In Neural Information Processing Systems},
  pages={712--720},
  year={2016},
	paperpdf={6267-regularized-nonlinear-acceleration.pdf},
	abstract={We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.}
}


@techreport{scieur2017integration,
  title={Integration Methods and Accelerated Optimization Algorithms},
  author={Scieur, Damien and Roulet, Vincent and Bach, Francis and d'Aspremont, Alexandre},
  journal={arXiv preprint arXiv:1702.06751},
  year={2017},
	paperpdf={Integration Methods and Accelerated Optimization Algorithms.pdf},	
	abstract={We show that accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. In comparison with recent advances in this vein, the differential equation considered here is the basic gradient flow and we show that multi-step schemes allow integration of this differential equation using larger step sizes, thus intuitively explaining acceleration results.}
}

@techreport{scieur2017nonlinear,
  title={Nonlinear Acceleration of Stochastic Algorithms},
  author={Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
  journal={arXiv preprint},
  year={2017},
	paperpdf={Nonlinear Acceleration of Stochastic Algorithms.pdf},	
	abstract={Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.}
}


@mastersthesis{scieur2015master,
  title={Global Complexity Analysis For The Second-Order Methods},
  author={Scieur, Damien},
  year={2015},
  month = {jun},
  school = {Universit\'e catholique de Louvain},
	paperpdf={masterthesis_scieur_damien.pdf},
	abstract={The goal of this master thesis will be firstly to analyze with precision the behavior of the cubic regularization of Newton's method on strongly convex functions. During this analysis we will find that the regularisation does not work as well as expected when the function is also smooth, unlike the gradient method. We will thus propose some variants of the original algorithm in order to have better global performances.}
}
