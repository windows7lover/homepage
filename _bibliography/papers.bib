---
---
References
==========


@inproceedings{scieur2016regularized,
  title={Regularized Nonlinear Acceleration},
  author={Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
  booktitle={Advances In Neural Information Processing Systems},
  pages={712--720},
  year={2016},
	paperpdf={6267-regularized-nonlinear-acceleration.pdf},
	abstract={We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.}
}


@techreport{scieur2017integration,
  title={Integration Methods and Accelerated Optimization Algorithms},
  author={Scieur, Damien and Roulet, Vincent and Bach, Francis and d'Aspremont, Alexandre},
  journal={arXiv preprint arXiv:1702.06751},
  year={2017},
	paperpdf={Integration Methods and Accelerated Optimization Algorithms.pdf},	
	abstract={We show that accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. In comparison with recent advances in this vein, the differential equation considered here is the basic gradient flow and we show that multi-step schemes allow integration of this differential equation using larger step sizes, thus intuitively explaining acceleration results.}
}


@mastersthesis{scieur2016regularized,
  title={Regularized Nonlinear Acceleration},
  author={Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
  booktitle={Advances In Neural Information Processing Systems},
  pages={712--720},
  year={2016},
	paperpdf={6267-regularized-nonlinear-acceleration.pdf},
	abstract={We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.}
}
